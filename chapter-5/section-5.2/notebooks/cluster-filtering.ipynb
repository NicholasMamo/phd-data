{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "\n",
    "One problem with the lexicons is that they contain some popular event terms, such as the words *manchester* or *kepa*.\n",
    "This notebook filters both lexicons and lexical concepts (word clusters) using a systematic method.\n",
    "The way filtering works, this notebook removes words from the lexicons and lexical concepts that had been used as tracking keywords to collect the datasets that generated the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/GitHub/EvenTDT\"))\n",
    "\n",
    "meta = os.path.expanduser('~/DATA/analyses/tdt/meta') # tracking details for the datasets used to generate the lexicons\n",
    "\n",
    "filters = os.path.expanduser('~/DATA/analyses/tdt/filters-all.json') # the filters, a bootstrapping file\n",
    "new_filters = os.path.expanduser('~/DATA/analyses/tdt/filters-filtered.json') # the path where the new filters will be stored\n",
    "\n",
    "# splits = os.path.expanduser('~/DATA/analyses/tdt/splits-80.csv') # the concepts, saved as a CSV file with one split on each line\n",
    "splits = os.path.expanduser('~/DATA/analyses/tdt/concepts-15.json')\n",
    "new_splits = os.path.expanduser('~/DATA/analyses/tdt/concepts-filtered-15.csv') # the path where the new splits will be stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load all the tracking keywords.\n",
    "This notebook looks for tracking keywords used during the event, which contain the names of players, coaches and the stadium in addition to basic information.\n",
    "The next code cell tokenizes all tracking keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventdt.nlp import Tokenizer\n",
    "tokenizer = Tokenizer(stem=True) # stem the tokenizer\n",
    "\n",
    "all_tokens = set() # all the tokens used to collect datasets\n",
    "\n",
    "for file in os.listdir(meta): \n",
    "    with open(os.path.join(meta, file)) as f:\n",
    "        metadata = json.loads(''.join(f.readlines()))\n",
    "        keywords = metadata['event']['keywords'] # use the event keywords\n",
    "        all_tokens = all_tokens.union(set( [ token for keyword in keywords\n",
    "                                                   for token in tokenizer.tokenize(keyword) ] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell loads the bootstrapping's output and filters the seed terms and bootstrapped terms that had been used to collect datasets.\n",
    "\n",
    "Note that this notebook does not eliminate the keywords, but prepends *EXCLUDED-\\**.\n",
    "Because of stemming and splitting, these keywords will never be filtered or used for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering arsen\n",
      "Filtering manchest\n",
      "Filtering man\n",
      "Filtering kepa\n",
      "Filtering hold\n",
      "Filtering rob\n",
      "Filtering lacazett\n",
      "Filtering hazard\n",
      "Filtering willock\n",
      "Filtering silva\n",
      "Filtering foden\n",
      "Filtering anthoni\n",
      "Filtering mendi\n",
      "Filtering bellerin\n"
     ]
    }
   ],
   "source": [
    "with open(filters) as f1, open(new_filters, 'w') as f2:\n",
    "    data = json.loads(''.join(f1.readlines()))\n",
    "    for term in data['pcmd']['seed']:\n",
    "        if term in all_tokens:\n",
    "            print(f\"Filtering { term }\")\n",
    "    data['pcmd']['seed'] = [ f\"EXCLUDED-{ keyword }\" if keyword in all_tokens else keyword # filter seed terms\n",
    "                                                     for keyword in data['pcmd']['seed'] ]\n",
    "    \n",
    "    for term in data['bootstrapped']:\n",
    "        if term in all_tokens:\n",
    "            print(f\"Filtering { term }\")\n",
    "    data['bootstrapped'] = [ f\"EXCLUDED-{ keyword }\" if keyword in all_tokens else keyword # filter bootstrapped terms\n",
    "                                                     for keyword in data['bootstrapped'] ]\n",
    "    f2.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting is similar, although it uses a CSV file.\n",
    "The next code cell processes each split separately and filters the terms as above, by prepending *EXCLUDED-\\**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering manchest\n",
      "Filtering kepa\n",
      "Filtering arsen\n",
      "Filtering man\n"
     ]
    }
   ],
   "source": [
    "with open(splits) as f1, open(new_splits, 'w') as f2:\n",
    "    concepts = json.loads(''.join(f1.readlines()))['concepts']\n",
    "    writer = csv.writer(f2, delimiter=',')\n",
    "    for concept in concepts:\n",
    "        for term in concept:\n",
    "            if term in all_tokens:\n",
    "                print(f\"Filtering { term }\")\n",
    "\n",
    "        terms = [ term if term not in all_tokens else f\"EXCLUDED-{ term }\" for term in concept ] # filter the splits\n",
    "        writer.writerow(terms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
